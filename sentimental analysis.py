# -*- coding: utf-8 -*-
"""Untitled4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fRK-tTX39SKYohWEbPEcUFnNN8Irb7VK
"""

# Step 1: Install dependencies
!pip install transformers nltk pandas seaborn matplotlib --quiet

# Step 2: Import libraries
import pandas as pd
import re
import nltk
from nltk.corpus import stopwords
import seaborn as sns
import matplotlib.pyplot as plt
from transformers import pipeline

# Step 3: Download NLTK stopwords
nltk.download('stopwords')

# Step 4: Upload CSV file
from google.colab import files
uploaded = files.upload()

# Step 5: Load CSV and detect text column
df = pd.read_csv(next(iter(uploaded)))

def detect_text_column(df):
    text_lens = {}
    for col in df.select_dtypes(include='object').columns:
        avg_len = df[col].dropna().apply(lambda x: len(str(x).split())).mean()
        text_lens[col] = avg_len
    return max(text_lens, key=text_lens.get)

text_col = detect_text_column(df)
print(f"Detected tweet text column: {text_col}")
df = df.dropna(subset=[text_col])
df.rename(columns={text_col: 'Tweet'}, inplace=True)

# Step 6: Clean tweets
def clean_tweet(tweet):
    tweet = re.sub(r"http\S+|www\S+|https\S+", '', tweet)
    tweet = re.sub(r'@\w+|#', '', tweet)
    tweet = re.sub(r'[^A-Za-z\s]', '', tweet)
    tweet = tweet.lower()
    tweet = ' '.join([word for word in tweet.split() if word not in stopwords.words('english')])
    return tweet

df['Clean_Tweet'] = df['Tweet'].apply(clean_tweet)

# Step 7: Prepare text for classification (fallback to original if cleaned is empty)
def get_text_for_classification(row):
    return row['Clean_Tweet'] if row['Clean_Tweet'].strip() else row['Tweet']

df['Text_For_Emotion'] = df.apply(get_text_for_classification, axis=1)

# Step 8: Emotion classification
emotion_classifier = pipeline("text-classification", model="j-hartmann/emotion-english-distilroberta-base", return_all_scores=False)

def classify_emotion(text):
    try:
        return emotion_classifier(text)[0]['label']
    except:
        return 'error'

df['Emotion'] = df['Text_For_Emotion'].apply(classify_emotion)

# Step 9: Save and download the full result
output_filename = "tweet_emotions_output.csv"
df.to_csv(output_filename, index=False)
files.download(output_filename)

# Step 10: Plot emotion distribution
sns.countplot(data=df, x='Emotion', order=df['Emotion'].value_counts().index)
plt.title('Distribution of Emotions in Tweets')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

emotion_counts = df['Emotion'].value_counts()
plt.figure(figsize=(8, 8))
plt.pie(emotion_counts, labels=emotion_counts.index, autopct='%1.1f%%', startangle=140)
plt.title('Emotion Proportions in Tweets')
plt.axis('equal')  # Equal aspect ratio ensures the pie chart is circular.
plt.show()

df['Tweet_Length'] = df['Tweet'].apply(len)
sns.boxplot(data=df, x='Emotion', y='Tweet_Length', order=df['Emotion'].value_counts().index)
plt.title('Tweet Length Distribution by Emotion')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

df['Tweet_Length'] = df['Tweet'].apply(len)

plt.figure(figsize=(10, 6))
sns.histplot(df['Tweet_Length'], bins=30, kde=True, color='skyblue')
plt.title('Histogram of Tweet Lengths')
plt.xlabel('Tweet Length')
plt.ylabel('Frequency')
plt.tight_layout()
plt.show()

plt.figure(figsize=(12, 6))
sns.histplot(data=df, x='Tweet_Length', hue='Emotion', element='step', stat='density', common_norm=False)
plt.title('Tweet Length Distribution by Emotion')
plt.xlabel('Tweet Length')
plt.ylabel('Density')
plt.tight_layout()
plt.show()